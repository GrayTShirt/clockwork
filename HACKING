------------------------
Developing for Clockwork
------------------------

So you want to hack on Clockwork, eh?

Well, you're in luck.  I have looked at far too much open source
code that is a pain in the a** to dive into, with cryptic and
undocumented build systems.  I vowed long ago not to repeat that
mistake.




1. TOOLS YOU WILL NEED
----------------------

A Linux Development Environment

  Clockwork does not (at this time) support any platform other
  than Linux.  If you are interested in *porting* Clockwork to
  another platform, I'll gladly accept patches.

  (Work *is* underway to get Clockwork ported to the *BSD family)

  Otherwise, get a Linux box to develop on.  I strongly urge
  you to get a dedicated machine (it can be a virtual machine)
  to do your testing on.  As a configuration management system,
  Clockwork needs root access and with root access, bugs can
  wreak some serious havok on your build box.

An ANSI C99-compliant C compiler

  The policy master and agent components are written entirely in
  ANSI C99 C (at least its *supposed* to be ANSI C99).  The build
  environment uses several GCC-specific flags to catch code errors.

  I personally use GCC 4.4.3 out of the Ubuntu repositories

Supporting Library Development Headers

  Clockwork currently relies on the following 3rd party libraries:

  - Augeas          Used by several resources for config file mods
  - SQLite3         For the reporting database backend
  - OpenSSL         Provides the TLS security later

Supporting Software Development Tools

  The Clockwork build system relies on the following auxiliary
  tools to do its job:

  - GNU Flex/Bison   For config / manifest / template parsers
  - GCOV / LCOV      Code coverage profiler with HTML reports
  - Perl             Functional test suite is written entirely in Perl
  - cdoc             Documentation (see https://github.com/filefrog/cdoc)
  - expect           Functional Tests rely on expect for automation



2. SETTING UP THE ENVIRONMENT
-----------------------------

To get started, you'll want to configure your build environment.
We recommend:

  $ ./autogen.sh
  $ ./configure CFLAGS="-DDEVEL -g"



3. THERE IS NO SECTION 3
------------------------
(ha!)



4. CONTRIBUTING PATCHES
-----------------------

If you have worked up a bugfix or feature-enhancement patch for Clockwork,
email it to me.  The only thing I ask is that you:

 1. Use `diff -u -C5' to generate the patch

 2. Include test cases to exercise your code

 3. Describe your changes in the body of your email
    (not in the comments, unless necessary)

 4. Explicitly license your code under a BSD or MIT license.
    (I will add you to the CONTRIBUTORS file and retain your copyright notice)

My email address is (currently) james@jameshunt.us



5. Notes to Developers
----------------------

5.1. Locking System Accounts
----------------------------
User account locking seems to be accomplished via the standard passwd(1)
command by placing an exclamation point (!, 0x21) immediately before the
encrypted password in the /etc/shadow file.  Unlocking is done by removing
this exclamation point.  This method seems to be preferred because it
allows sysads to lock an account without losing the original password.

5.2. PWDB / SPDB - Remove List Head
-----------------------------------
Removing the list head (first entry in the database) currently presents a
few problems.  The caller has a pointer to the entry structure, so
stitching it out of the picture does not work.

Perhaps the best bet is to go back to pointers for payload (either struct
passwd * or struct spwd *) and set the pointer to NULL for all deleted
payloads (and free()ing memory accordingly).  Of course, the documentation
will have to warn callers that the second parameter passed to the pwdb_rm()
function will be freed, so that future usage results in undefined behavior.

SOLVED: The passwd member of struct pwdb is now a pointer
        The spwd member of struct spdb is now also a pointer

5.3. Object Lifecycle and Memory Management
-------------------------------------------
The internal APIs between modules (stringlist, res_*, policy, etc.) need
to exhibit consistent behavior with respect to object (struct) creation,
initialization, and memory management.  To adhere to the Principle of
Least Surprise, the following conventions must be upheld:

int OBJECT_init(OBJECT*, ...)
        Initializes the members of the OBJECT with sane defaults
        Does not perform memmory allocation
        Returns non-zero if initialization fails
        Accepts other arguments for initialization, as needed

OBJECT* OBJECT_new(...)
        Dynamically allocates memory for an OBJECT
        Calls OBJECT_init to initialize the OBJECT
        Returns NULL if allocation or initialization fails
        Accepts the same arguments as OBJECT_init, for initialization

void OBJECT_deinit(OBJECT*)
        De-initializes members of OBJECT
        OBJECT is in an undefined state after this call

void OBJECT_free(OBJECT*)
        Calls OBJECT_deinit to de-initialize the OBJECT
        Frees dynamically allocated memory

These functions operate in pairs:
        OBJECT_init ----> (do stuff with object) ---> OBJECT_deinit
        OBJECT_new  ----> (do stuff with object) ---> OBJECT_free

Additionally, the following guidelines with respect to memory management
should be followed whenever possible, unless there is good reason not to.

 1. An object is responsible for all memory it allocates

        If a module (like stringlist) allocates an area of memory, either
        because it was asked to (stringlist_add) or as part of its integral
        operation, that module is responsible for freeing that memory.  It
        should not be left to the caller to fre memory allocated on their
        behalf.

 2. An object must not take responsibility for memory areas presented to
        it by callers.

        Callers should be able to live by guideline 1, above.  Ergo, modules
        should not take responsibility for memory away from a caller.  For
        example, stringlist_add takes a pointer to a null-terminated character
        array as its first argument, and adds it a list.  The module must
        allocate a copy of that string for storage in the stringlist struct,
        thereby leaving the caller responsible for the pointer they passed.

(These guidelines were laid down to maintain clearer code, and ensure that
memory leaks are easier to find, since responsibility for memory management
is clear and obvious.)

5.4 PACK As An Alternative to serializer/unserializer
-----------------------------------------------------

The serializer / unserializer interface (serialize.o) is a bit clunky.  In
essence, it is a stupid string parser with some numeric conversion routines
built in.  The biggest problem is the workload placed on the user of the
serialize module; serialize knows nothing about the actual contents of a
serialized string, and the API forces the caller to build up or pull apart
a serialized representation by hand.

Let's take a lesson from the Perl playbook, and implement a (slightly) simpler
version of the pack / unpack pair of functions.  Variadic functions can be
made so that the API is familiar to C programmers used to printf and scanf.

Example: packing a res_user object ...

  if (pack("aaLLaaaCaCLLLLL",
           ru->ru_name, ru->ru_passwd, ru->uid, ru->gid,
           ru->gecos, ru->shell, ru->mkhome, ru->skel,
           ru->lock, ru->pwmin, ru->pwmax, ru->pwwarn,
           ru->inact, ru->expire) != 0) {

        return -1; /* some sort of failure code */
  }

... and then unpacking it ...

  if (unpack("aaLLaaaCaCLLLLL",
             &ru->ru_name, &ru->ru_passwd, &ru->uid, &ru->gid,
             &ru->gecos, &ru->shell, &ru->mkhome, &ru->skel,
             &ru->lock, &ru->pwmin, &ru->pwmax, &ru->pwwarn,
             &ru->inact, &ru->expire) != 0) {

        return -1; /* some sort of failure code */
  }


5.5. Fact Gathering Through External Scripts
--------------------------------------------
The initial design of the fact module was to implement all fact gatherers
as C routines inside of the client-side agent.  After implementing uname through
the eponymous system call, and then adding LSB support via file parsing, it seems
it would be easier and more flexible to use independent scripts.

In this architecture, the fact gatherer would only need to exec multiple binaries
(which could be Perl, Ruby or Bash scripts) and parse the output.  The parsing
routines only need to be implemented once for the gatherer.  Since more scripts could
be added later (perhaps as part of a policy) this approach gains flexibility.

If the speed / optimization of C over external command interpreters is a concern,
fact gatherers could be implemented as standalone C programs that speak the same
serialization protocol that the gatherer understands.


5.6. Testing System-Level Remediation
-------------------------------------
Configuration management software is difficult to write tests for because it strives
to update system-level configuration items like user accounts and software packages.

Functional tests need to be written to verify that, given a current system state, and
a policy definition, the software can correctly bring the system to its target state.
In order to do this many times, with different starting system states, we need to
use some sort of temporary system state, set up by the test harness, for each test.

The main problem lies in the conflicting desires of the tester / programmer; the
software must be tested, but the development system should not be modified in the
process.

Throw-away chroot environments may be the answer: each test scenario can generate a
new root directory (probably under test/) and set it up with all of the "system-level"
files and configurations it needs to simulate as the "current state".  The test harness
can then chroot into the new root directory and execute in a sandbox.

Question: can normal users chroot?


5.7. Sending Generated Policy vs. Sending Policy Generators
-----------------------------------------------------------
During design / development of clockd, I had to decide whether to generate the policy
every time a client connects, and send the full policy definition, or just send the
syntax tree that the client could run against its set of facts and get a policy.

Here is a list of pros and cons for each approach

Option 1: Send the Full Policy

  Pros                                           Cons
  ============================================   ======================================
  Need-to-know: Client only gets the policy      More Complex Server: Policy Master
     that they need, and nothing more.              must be able to generate a policy
                                                    from a syntax tree, based on the
  Simpler Client: Agent logic only needs to         facts given by the client
     know how to receive and enforce

  Less Data Transfer (potentially): Client
     does not need to send its facts to the
     policy host.

  Pack Code Done: The pack / unpack code for
     a policy and its constituent resources
     is already written and tested.


Options 2: Send the Policy Generator

  Pros                                           Cons
  ============================================   ======================================
  Simpler Server: Policy Master only needs to    More Complex Client: Agent logic needs
     be able to map a host to a syntax tree         to be able to traverse a syntax tree
     root node.                                     and create a policy / resources.

  Less Data Transfer (potentially): If policy    Packing Nodes: Need to write code to
     definition has not changed, but facts          pack and unpack a syntax tree for
     have (locally) there is no need to send        wire transfer.  This will require
     a new policy to the client.                    pointer swizzling.

                                                 Information Leaks: Client will get entire
                                                    syntax tree, complete with alternate
                                                    conditionals.


Given the above list, it seems that sending the full policy, generated server-side,
has the most positive benefits and the fewest downsides.


5.8. File Remediation and the Remote File Server
------------------------------------------------
File Resources are "fixed" during policy enforcement (client-side) differently than
most other resources; primarily, the contents of the file are really only known to
the policy master, for security reasons.  To my knowledge, all other resource types
are atomic -- everything the client needs to remediate is contained directly in the
res_* structure, and retrieved from the policy master in the GET_POLICY PDU.

My original thought was to stat all res_file objects immediately after GET_POLICY,
and send out GET_FILE PDUs to the policy master for files that were non-compliant.
Unfortunately, this necessitates storing the contents of the files in temporary
areas, considering that some files could be very large.  This introduces a subtle
vulnerability if the temporary storage area is not 100% secure -- an attacker could
poison the files in the storage area before the client can remediate, essentially
spoofing file contents from the master.  This is not good.

Another idea was to implement a standalone (or built-in-but-separate) file server /
content server that could dole out files on demand.  In theory, it works well, and
provides a neat and clean separation of duties.  In practice, it brings in way too
many security access problems, efficiency concerns.  Thornier issues crop up when
you start getting into templated, dynamic files generated from client host facts.

Instead, why not keep the connection to the policy master open until the client
has (a) figured out which files need content remediation and (b) gets those files
from the server.  Since I eventually intend to add in post-run reporting, this
will work.


5.9. Declarative vs. Imperative vs. Both
----------------------------------------
If you read through any of the literature on popular configuration management sytems
like CFengine and Puppet, you will see that the authors have a real problem with
imperative methods of configuration management, and an affinity for the declarative
approach.  You tell the tool what you want, and it determines the best sequence of
actions to take to get the system there.

As awesome as that concept is, it doesn't track too well with people.  We are
imperative, task-based creatures.  "I want Z, so I should do X and then Y"

One of my biggest complaints with these existing systems is the mental workout
required to unroll a declarative configuration into its imperative format.  Most
of these systems do their own implicit dependency resolution.  If you say you
want the file "/some/path/to/file" to be owned by the user joe in group staff,
the tools puts in the dependency that "/some/path/to" is a directory, that it exists,
user joe needs to be created first, and so does the group 'staff'.  When those
dependencies are impossible to deduce, the system falls back to explicit dependency
specification from the config.  For example, Puppet lets you "notify" a service when
a file changes.  That's a fancy way of saying 'restart service B after updating
file A'.

Clockwork will (and does) embrace the declarative approach.  However, I must draw
the line at coming up with terms that don't sound imperative, when in fact they are.
If you need a resource to be evaluated after another, then the language will be
'after', not 'requires'.  It's just easier to grok.


5.10. Resource Trees
--------------------
Now that the declarative imperative tirade is over, here's some notes on Resouce Trees.
Resource Trees represent the implicit and explicit dependecies in a configuration.
We could just ignore these dependencies and hope that successive runs get us closer
and closer to the configuration, but that's a real disservice to system administrators.
I personally don't like the "getting closer, try again later" approach, so we're stuck
with dependency resolution.

If we take the same approach with resource trees as we did with syntax trees, this should
be pretty easy.  Define a struct rtree that contains:

 * a pointer to a resource (don't know what type, don't care)
 * a list of dependent resources (as struct rtrees).

(Astute comp sci grads will notice that this is a run-of-the-mill non-binary tree.)
"Free" resources, that have no further dependencies exist as leaf nodes.  These go through
remediation first, followed by their parents.  Consider the following graph, with each
letter representing a resource.

   A -- B -- D
   |     `-- E
   |
    `-- C -- F
        |--- G
         `-- H

In this example, resource C cannot go through remediation until D and E are in a good
state.  (D and E may be the user joe and group staff from the file example (B) above.)
Remediation could proceed in either of the following sequences (ignoring order variations
at the leaf node)

H, G, F, C, E, D, B, A (remediating C first)
D, E, B, F, G, H, C, A (remediating B first)

Order does not matter; only parentage affects the outcome.

As an implementation side note, the resource tree is only useful during remediation, not
after.  Therefore, the act of traversing the tree can be implemented as a destructive
operation if it simplifies the traversal logic.  For example, after remediating D, the
D node can be removed, leaving the tree as such:

   A -- B -- *
   |     `-- E
   |
    `-- C -- F
        |--- G
         `-- H

Three operations later, after remediating F:

   A -- * -- *
   |     `-- *
   |
    `-- C -- *
        |--- G
         `-- H

This could be done through rtree delete/free, or by flagging the rtree node as 'done'

Another sidenote: the policy master really should be able to identify cycles in the graph
(i.e. A depends on B, which itself depends on A).  However, without client facts, this
may not be possible.  In any event, someone needs to catch these situations before
attempting to traverse the tree for remediation.  A descriptive error message followed by
exit would be just the ticket.

5.11. An Alternative Aproach to Dependencies
--------------------------------------------

After further reflection (and an attempted implementation), it doesn't seem like dep trees
have as large an application as the above notes indicate.  Here's a few examples:

File dependencies::

If the policy enforces a file at "/path/to/file.dat" then res_file_remediate should be able
to create the directory path '/path/to' if all or part of it does not exist.  For security
reasons, missing directory components should be created according to the local umask, and
owned by root:root.

File Owner / Group dependencies::

As long as users and groups get fixed before files, the pwdb and grdb structures will hold
references to all users and groups known and created.  File remediation will then be able
to look up the owner UID and GID from pwdb and grdb.

User / Group dependencies::

Users depend on groups, but groups do not depend on users.  A user's primary group must
be created so that it can be looked up during user remediation.  For group memberships,
/etc/group and /etc/shadow only contain user names, not UIDs.  Therefore, if we remediate
groups before users, we can get around dependency graphing.

So...

The final decision (at least at this point) is to run the following sequence:
  Groups -> Users -> Files


6. Platform Support Matrix
--------------------------

OS/Distro       Version         Arch            Status
=========       =======         ====            ======
Ubuntu          11.04           i386            WORKING
CentOS          5.7             x86_64          WORKING
FreeBSD         8.2             ?               planning
OpenBSD         4.9             ?               planning
NetBSD          5.1             ?               planning
Fedora          Core 15         ?               HOLD
Gentoo          2011.09?        ?               HOLD


7. Resource Implementation Status
---------------------------------

Key:  . = detection
      _ = (space) outstanding
      + = detection + remediation



7.1. res_file (done)
--------------------
+ owner
+ group
+ mode
+ source / contents

7.2. res_dir (done)
-------------------
+ owner
+ group
+ mode

7.3. res_exec (done)
--------------------
+ command
+ test
+ user
+ group

7.4. res_package
----------------
+ name
+ status
+ version requirements


  NOTE: For res_package, it will be best to rely on the
        package manager of choice for the local platform.  For
        RedHat, use yum or up2date.  For Deb/Ubu, use apt*.
        Trying to integrate with these package manager DBs
        via C APIs seems to be too much work for little gain.

  DEBIAN:
          check installed:   "dpkg-query -W -f='${Version}' %s"
          install:           "apt-get install -qqy %s"
          install version:   "apt-get install -qqy %s=%s"
          erase:             "apt-get purge   -qqy %s"

  REDHAT:
          check installed:   "rpm --quiet --qf='%%{VERSION}-%%{RELEASE}' -q %s"
          install:           "yum install -qy %s"
          install version:   "yum install -qy %s-%s"
          erase:             "yum erase   -qy %s"

  GENTOO:
          check installed:   ???
          install:           ???
          install version:   ???
          erase:             ???

7.5. res_user
-------------
+ username
+ id
+ primary group
  groups
+ password
+ shell
+ home
+ make home / skeleton
+ pwmin
+ pwmax
+ pwwarn
+ expire
+ inactive
+ locked
+ comment

7.6. res_group (done)
---------------------
+ name
+ id
+ members
+ admins
+ password

7.7. res_service (done)
-----------------------
+ name
+ running / stopped
+ enabled / disabled

7.8. res_mount
--------------
  device (fs_spec)
  mountpoint (fs_file)
  filesystem (fs_vfstype)
  options (fs_mntops)
  dump (fs_freq)
  pass (fs_passno)

7.9. res_ssh_public_key
-----------------------
  file (target)
  user (target)
  comment
  type
  key

7.10. res_ssh_known_host
------------------------
  file (where to store it)
  host
  ip
  type
  key
  encrypt (boolean)

7.11. res_cron
--------------
  name
  minute
  hour
  mday (month_day)
  date
  wday (week_day)
  frequency (i.e. "@reboot")
  user
  system (boolean)
  command

7.12. res_host (done)
---------------------
+ hostname
+ aliases

7.13. res_net_interface
-----------------------
  device
  ip
  netmask
  gateway
  (sure there are others...)

7.14. res_net_route
-------------------
  network
  gateway
  (sure there are others... metric?)

7.15. res_sysctl (done)
-----------------------
+ name
+ value
+ persist

8. Clockwork Specification Language
-----------------------------------

This section explains the design of the Clockwork specification file
format, with a focus on implementation of the lexer / parser as a reentrant
Flex / Bison compiler.

Clockwork specification files server two purposes:

  1. Define Policies as a set of resources (based on host facts)
  2. Identify which policy or policies apply to which host(s)

Specifications are written in a declarative, block-based language.  Policies
are defined in a block like this:

  # Comments start with a '#' and continue to the
  # end of the line, similar to bash, Perl and Ruby
  policy "sample-policy" {
    # Resource are defined here, inside the curly braces

    group "staff" {
      gid: 2140
    }

    user "jrhunt" {
      uid:  701
      gid:  2140
      home: "/home/staff/jrhunt"
    }

  }

The above policy, named "sample-policy", defines two resources: a group
named staff and a user named jrhunt.  Each resource definition is itself a
block.

Resources must be defined inside of a policy; they cannot be defined
globally, nor can they be nested inside of other resources.  Policies can
only be defined at the top level (globally); they cannot nest.

Attributes (which belong exclusibely to resource block) are defined in the
form attribute_name: "value of attribute", where <attribute_name> is some
keyword understood by the underlying resource type, and "value of attribute"
is an appropriate value for the attribute.

Conditionals are supported in two forms: if blocks at the policy level, and
map blocks at the attirbute value.  Consider the following:

  policy "conditional" {

    # Only define the "ubuntu" user on Ubuntu installations
    if (lsb.distro.id is "Ubuntu") {
      user "ubuntu" {
        home: "/srv/oper/ubuntu"
      }
    }

    file "/etc/sudoers" {
      mode:  0600
      owner: "root"
      group: "root"

      source: map(sys.local.environment) {
        "test": "/base/sudoers-loose"
        "dev":  "/base/sudoers-loose"
        "prod": "/base/sudoers-strict"
      }
    }

  }

The 'if' conditional block surrounding the "ubuntu" user definition ensures
that the user will only be defined in the host fact "lsb.distro.id" is equal
to the string "Ubuntu" (via case-sensitive string comparison).  If ths
policy is applied to a Fedora or CentOS installation, the if test will fail,
and the user resource will not be defined.

Similarly, the map construct inside the "/etc/sudoers" file resource
definition modifies the source attribute based on the current environment
stored in the "sys.local.environment" fact.  For test and development boxes,
the file is sourced from /base/sudoers-loose, a config that may allow
developers to run specific commands that are normally the purview of root.
In production, however, a different sudoers file is used, probably one that
prevents developers from doing anything.

While the two conditional forms are similar in intent, they vary in
applicability.  An if block can only occur inside of a policy block, or
nested inside of another if block.  The map construct can only appear in
place of a static attribute value.  Therefore, these two configurations are
semantically invalid:

  policy "invalid-map" {

    map(lsb.distro.id) {
      "Ubuntu": user "ubuntu" {
        home: "/srv/oper/ubuntu"
      }
    }

  }

and

  policy "invalid-if" {

    file "/etc/sudoers" {
      mode:  0600
      owner: "root"
      group: "root"

      source: if (sys.local.environment is "prod") {
        "/base/sudoers-strict"
      }
    }

  }

Both conditional forms understand the idea of 'else', some control path that
should be followed if the conditions presented do not hold.  For if blocks,
the semantics are identical to most programming languages:

  policy "if-else" {

    # on Ubuntu hosts, define UID 42 as 'ubuntu'
    if (lsb.distro.id is "Ubuntu") {
      user "ubuntu" {
        uid:  42
        home: "/srv/oper/ubuntu"
      }
    } else { # everywhere else, define it as  'linux'
      user "linux" {
        uid:  42
        home: "/srv/oper/linux"
      }
    }

  }

As the coments indicate, on Ubuntu boxes, user 42 will be named "ubuntu",
and everywhere else (Gentoo, Debian, Slack) it will be "linux".

The map construct works similarly, supplying a default attribute value if
none of the test values match the actual value:

  policy "map-with-else" {

    file "/etc/sudoers" {
      mode:  0600
      owner: "root"
      group: "root"

      source: map(sys.local.environment) {
        "test": "/base/sudoers-loose"
        "dev":  "/base/sudoers-loose"
        "prod": "/base/sudoers-strict"
        else:   "/base/sudoers-untrusted"
      }
    }

  }

The final value case maps "/base/sudoers-untrusted" as the source for
/etc/sudoers on any host that does not have a sys.local.environment of
test, dev or prod.  This includes the edge case where the fact is not even
defined.

NOTE: There is currently no way to chain multiple if blocks together in an
if-else-if-else idiom.  Nested ifs provide a temporary work-around to this
problem, but the long-term solution is still awaiting implementation.

8.1. Implementation Details
---------------------------

The specification parser is completely contained inside the spec/
subdirecory of the project root.  It is implemented as a reentrant Flex
lexical analyzer paired with a reentrant (pure) Bison LALR parser.

The documentation on reentrancy in Flex and Bison, both on the 'net and in
_lex & yacc 2nd Edition_ (Levine, Mason and Brown; O'Reilly) is sparse at
best.  The remainder of this document attempts to collect my notes on the
implementation.


8.1.1. Filesystem Layout
------------------------

To keep things clean, the parser implementation is split out into multiple
files, each with a distinct purpose:

  lexer.l         Flex lexical analysis rules

  lexer_impl.h    Supporting functions for lexer.l.  This file is never
                  compiled outright; it is included in the lexer.l generated
                  code, and contains implementation details like
                  error-handling routines.

  grammar.y       Bison LALR grammar rules

  grammar_impl.h  Supporting functions for grammar.y.  This file is never
                  compiled outright; it is included in the grammar.y
                  generated code, and contains static routines of interest
                  only to the Bison parser.

  parser.h        Public spec parser API header file containing visible
                  functions for interacting with the parser subsystem.

  private.h       Private spec parser header file (used only by lexer.l and
                  grammar.y generated code, when a function needs to be
                  available to both components

  parser.c        Implements the public parser API (as defined by parser.h).
                  This file does not contain any function definitions that
                  are internal to the lexer / parser (see lexer_impl.h and
                  grammar_impl.h, respectively).

These files are compiled to create the following object files:

  lexer.o         Combination of lexer.c (as generated from lexer.l) and
                  lexer_impl.h as a single translation unit.  Represents the
                  lexical analyzer module.

  grammar.o       Combination of grammar.c (as generated from grammar.y) and
                  grammar_impl.h as a single translation unit.  Represents
                  the LALR parser module.

  parser.o        Compilation of parser.c; the public API necessary to drive
                  the lexer / parser.

These object files cannot stand on their own.  Any program wishing to make
use of the parser must contain all three.


8.1.2. Public API Overview
--------------------------

The public API for the parser subsystem consists of one function:

  struct ast* parse_file(const char *path);

This function opens path in read-only mode and parses it.  A pointer to the
resulting abstract syntax tree is returned.  If unrecoverable errors are
encountered, NULL is returned.
