Locking System Accounts
-----------------------
User account locking seems to be accomplished via the standard passwd(1)
command by placing an exclamation point (!, 0x21) immediately before the
encrypted password in the /etc/shadow file.  Unlocking is done by removing
this exclamation point.  This method seems to be preferred because it
allows sysads to lock an account without losing the original password.

PWDB / SPDB - Remove List Head
------------------------------
Removing the list head (first entry in the database) currently presents a
few problems.  The caller has a pointer to the entry structure, so
stitching it out of the picture does not work.

Perhaps the best bet is to go back to pointers for payload (either struct
passwd * or struct spwd *) and set the pointer to NULL for all deleted
payloads (and free()ing memory accordingly).  Of course, the documentation
will have to warn callers that the second parameter passed to the pwdb_rm()
function will be freed, so that future usage results in undefined behavior.

SOLVED: The passwd member of struct pwdb is now a pointer
        The spwd member of struct spdb is now also a pointer

Object Lifecycle and Memory Management
--------------------------------------
The internal APIs between modules (stringlist, res_*, policy, etc.) need
to exhibit consistent behavior with respect to object (struct) creation,
initialization, and memory management.  To adhere to the Principle of
Least Surprise, the following conventions must be upheld:

int OBJECT_init(OBJECT*, ...)
	Initializes the members of the OBJECT with sane defaults
	Does not perform memmory allocation
	Returns non-zero if initialization fails
	Accepts other arguments for initialization, as needed

OBJECT* OBJECT_new(...)
	Dynamically allocates memory for an OBJECT
	Calls OBJECT_init to initialize the OBJECT
	Returns NULL if allocation or initialization fails
	Accepts the same arguments as OBJECT_init, for initialization

void OBJECT_deinit(OBJECT*)
	De-initializes members of OBJECT
	OBJECT is in an undefined state after this call

void OBJECT_free(OBJECT*)
	Calls OBJECT_deinit to de-initialize the OBJECT
	Frees dynamically allocated memory

These functions operate in pairs:
	OBJECT_init ----> (do stuff with object) ---> OBJECT_deinit
	OBJECT_new  ----> (do stuff with object) ---> OBJECT_free

Additionally, the following guidelines with respect to memory management
should be followed whenever possible, unless there is good reason not to.

 1. An object is responsible for all memory it allocates

	If a module (like stringlist) allocates an area of memory, either
	because it was asked to (stringlist_add) or as part of its integral
	operation, that module is responsible for freeing that memory.  It
	should not be left to the caller to fre memory allocated on their
	behalf.

 2. An object must not take responsibility for memory areas presented to
	it by callers.

	Callers should be able to live by guideline 1, above.  Ergo, modules
	should not take responsibility for memory away from a caller.  For
	example, stringlist_add takes a pointer to a null-terminated character
	array as its first argument, and adds it a list.  The module must
	allocate a copy of that string for storage in the stringlist struct,
	thereby leaving the caller responsible for the pointer they passed.

(These guidelines were laid down to maintain clearer code, and ensure that
memory leaks are easier to find, since responsibility for memory management
is clear and obvious.)

PACK As An Alternative to serializer/unserializer
-------------------------------------------------

The serializer / unserializer interface (serialize.o) is a bit clunky.  In
essence, it is a stupid string parser with some numeric conversion routines
built in.  The biggest problem is the workload placed on the user of the
serialize module; serialize knows nothing about the actual contents of a
serialized string, and the API forces the caller to build up or pull apart
a serialized representation by hand.

Let's take a lesson from the Perl playbook, and implement a (slightly) simpler
version of the pack / unpack pair of functions.  Variadic functions can be
made so that the API is familiar to C programmers used to printf and scanf.

Example: packing a res_user object ...

  if (pack("aaLLaaaCaCLLLLL",
           ru->ru_name, ru->ru_passwd, ru->uid, ru->gid,
           ru->gecos, ru->shell, ru->mkhome, ru->skel,
           ru->lock, ru->pwmin, ru->pwmax, ru->pwwarn,
           ru->inact, ru->expire) != 0) {

        return -1; /* some sort of failure code */
  }

... and then unpacking it ...

  if (unpack("aaLLaaaCaCLLLLL",
             &ru->ru_name, &ru->ru_passwd, &ru->uid, &ru->gid,
             &ru->gecos, &ru->shell, &ru->mkhome, &ru->skel,
             &ru->lock, &ru->pwmin, &ru->pwmax, &ru->pwwarn,
             &ru->inact, &ru->expire) != 0) {

        return -1; /* some sort of failure code */
  }


Fact Gathering Through External Scripts
---------------------------------------
The initial design of the fact module was to implement all fact gatherers
as C routines inside of the client-side agent.  After implementing uname through
the eponymous system call, and then adding LSB support via file parsing, it seems
it would be easier and more flexible to use independent scripts.

In this architecture, the fact gatherer would only need to exec multiple binaries
(which could be Perl, Ruby or Bash scripts) and parse the output.  The parsing
routines only need to be implemented once for the gatherer.  Since more scripts could
be added later (perhaps as part of a policy) this approach gains flexibility.

If the speed / optimization of C over external command interpreters is a concern,
fact gatherers could be implemented as standalone C programs that speak the same
serialization protocol that the gatherer understands.


Testing System-Level Remediation
--------------------------------
Configuration management software is difficult to write tests for because it strives
to update system-level configuration items like user accounts and software packages.

Functional tests need to be written to verify that, given a current system state, and
a policy definition, the software can correctly bring the system to its target state.
In order to do this many times, with different starting system states, we need to
use some sort of temporary system state, set up by the test harness, for each test.

The main problem lies in the conflicting desires of the tester / programmer; the
software must be tested, but the development system should not be modified in the
process.

Throw-away chroot environments may be the answer: each test scenario can generate a
new root directory (probably under test/) and set it up with all of the "system-level"
files and configurations it needs to simulate as the "current state".  The test harness
can then chroot into the new root directory and execute in a sandbox.

Question: can normal users chroot?


Sending Generated Policy vs. Sending Policy Generators
------------------------------------------------------
During design / development of policyd, I had to decide whether to generate the policy
every time a client connects, and send the full policy definition, or just send the
syntax tree that the client could run against its set of facts and get a policy.

Here is a list of pros and cons for each approach

Option 1: Send the Full Policy

  Pros                                           Cons
  ============================================   ======================================
  Need-to-know: Client only gets the policy      More Complex Server: Policy Master
     that they need, and nothing more.              must be able to generate a policy
                                                    from a syntax tree, based on the
  Simpler Client: Agent logic only needs to         facts given by the client
     know how to receive and enforce

  Less Data Transfer (potentially): Client
     does not need to send its facts to the
     policy host.

  Pack Code Done: The pack / unpack code for
     a policy and its constituent resources
     is already written and tested.


Options 2: Send the Policy Generator

  Pros                                           Cons
  ============================================   ======================================
  Simpler Server: Policy Master only needs to    More Complex Client: Agent logic needs
     be able to map a host to a syntax tree         to be able to traverse a syntax tree
     root node.                                     and create a policy / resources.

  Less Data Transfer (potentially): If policy    Packing Nodes: Need to write code to
     definition has not changed, but facts          pack and unpack a syntax tree for
     have (locally) there is no need to send        wire transfer.  This will require
     a new policy to the client.                    pointer swizzling.

                                                 Information Leaks: Client will get entire
                                                    syntax tree, complete with alternate
                                                    conditionals.


Given the above list, it seems that sending the full policy, generated server-side,
has the most positive benefits and the fewest downsides.


File Remediation and the Remote File Server
-------------------------------------------
File Resources are "fixed" during policy enforcement (client-side) differently than
most other resources; primarily, the contents of the file are really only known to
the policy master, for security reasons.  To my knowledge, all other resource types
are atomic -- everything the client needs to remediate is contained directly in the
res_* structure, and retrieved from the policy master in the GET_POLICY PDU.

My original thought was to stat all res_file objects immediately after GET_POLICY,
and send out GET_FILE PDUs to the policy master for files that were non-compliant.
Unfortunately, this necessitates storing the contents of the files in temporary
areas, considering that some files could be very large.  This introduces a subtle
vulnerability if the temporary storage area is not 100% secure -- an attacker could
poison the files in the storage area before the client can remediate, essentially
spoofing file contents from the master.  This is not good.

The second idea was to keep the connection to the policy master open all the way
through remediation, and ask the server for files via GET_FILE/SEND_FILE as
necessary.  This would require not only keeping the TCP/SSL connection alive, but
also modifying the res_file implementation to understand the core protocol.
Considering that some remediation (i.e. the forthcoming res_package implementation)
could take a long time to complete, keeping the connection open seems sub-optimal.
On top of that, the coupling of proto.o and res_file.o doesn't sit right with me.

Instead, why don't we implement a file server that can hand out file contents and
SHA1 checksums.  The policy master can then ask the file server for checksums,
based on remote source path, and the clients can ask the file server for contents,
based on remote checksums.  While this complicates the deployment slightly (admins
will have to manage a few different daemons) it provides more deployment options
and flexibility; i.e. file server on a different machine (or set of machines) than
the policy master.

That brings us to the sticky problem of dynamic file contents.  Ideally, res_file
will be able to generate files for clients based on their facts.  For example, the
sshd_config for OpenSSH sshd could be tailored so that sshd only listens on the
first network interface.  For each host, a different file.  Without the ability to
generate a template, and insert the IP address of eth0 where needed, administrators
need to proactively populate clockwork with tons of static files.  This then leads
to hard-coded IP addresses.

So how do we reconcile the need for files that change based on client facts?  The
file server should not need to know about client facts -- to be flexible, it has
to be dumb.

One idea is to have the policy master generate the file, the checksum, store the
file in the content server and send the checksum to the client.  This brings in a
temporary storage area, but honestly, what is the difference between a temporary
area dedicated to storing generated files and a directory of files?  If an attacker
has compromised the box, they have access to both areas (and memory).

Do we even need to split out the file server?  Probably not.  From a distributed
deployment scenario, one would need multiple policy servers anyway, so why not
implement the file server inside of policyd?  In a distributed deployment, the
policy server could replicate the policies, static files and static templates.
Files would only be valid after a policy had been generated.

Final design decision: implement the file / content server as part of policyd.

Access Control: How do we keep clients isolated, so that host A cannot request the
information intended solely for host B?  For policy definitions, this is done by
the hosts member of a manifest structure; when a host connects and issues a
GET_POLICY, policy looks at manifest->hosts[HOSTNAME] to generate the policy.
We could extend the idea, so that a manifest contains a unique set of pointers, for
each host, into a master file server table.
